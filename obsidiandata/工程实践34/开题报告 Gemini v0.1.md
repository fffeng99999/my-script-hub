### **工程实践课程开题报告书**

**题　　目**：基于文本引导与分层渐进式Transformer的图像超分辨率重建

**组　　号**：(请在此填写你们的组号)

**组员分工**：

| 组员名称  | 学号          | 分工                                                                             |
| :---- | :---------- | :----------------------------------------------------------------------------- |
| (组员A) | 202XXXXXXXX | **负责人、算法设计**：负责项目整体协调，主导 Transformer 核心模型结构设计与创新点探索，并负责最终的答辩汇报。                |
| (组员B) | 202XXXXXXXX | **代码实现与训练**：负责将设计的模型（包括三个阶段）转化为代码实现，构建并优化完整的训练与推理流程，负责模型调优。                    |
| (组员C) | 202XXXXXXXX | **数据与评估**：负责数据集（如 Set5, Urban100）的准备、预处理与加载，实现性能评估指标（PSNR, SSIM），并系统记录和分析实验结果。 |
| (组员D) | 202XXXXXXXX | **文档与可视化**：负责撰写项目技术文档（学习记录、实验报告），开发可视化模块（如GIF生成），并协助准备答辩PPT材料。                 |
|       |             |                                                                                |


---

### **第 1 章 立项依据**

#### **1.1 项目背景**

图像超分辨率（Super-Resolution, SR）重建技术旨在从低分辨率（LR）图像中恢复出高分辨率（HR）图像，是计算机视觉领域的关键技术之一。随着高清显示设备的普及，该技术在**医疗成像、卫星遥感、视频监控和移动端影像增强**等场景中显示出巨大的应用价值 。

早期 SR 方法多依赖于插值或传统重建模型，效果有限。近年来，深度学习，特别是**卷积神经网络（CNN）和生成对抗网络（GAN）**，极大地推动了 SR 技术的发展。然而，CNN 由于其固有的局部感受野限制，在捕捉图像长距离依赖和全局结构一致性方面存在瓶颈。**变换器网络（Transformer）**凭借其自注意力机制，能够有效建模全局上下文关系，在自然语言处理领域取得了革命性成功后，也开始在视觉任务中展现出卓越性能。SwinIR 等模型已证明，Transformer 架构能够有效提升 SR 的重建质量。

本项目旨在以上述研究为基础，设计并实现一个以 Transformer 为核心的创新性图像超分辨率模型。

#### **1.2 基本原理**

本项目的核心技术思路是构建一个**分层渐进式**的超分辨率模型，该模型借鉴了人类“先轮廓、再纹理、后语义”的认知过程。我们将整个 SR 任务分解为三个独立的、循序渐进的阶段：

1. **轮廓恢复层 (Contour Layer)**：此阶段专注于恢复图像的基础结构和边缘信息。模型接收低分辨率输入，通过一系列卷积操作进行初步的、平滑的放大，其优化目标是最小化与高分辨率图像在像素级别上的 L1 损失。
2. **纹理增强层 (Texture Layer)**：在前一阶段输出的轮廓图像基础上，此阶段致力于补充丰富的细节和纹理。我们引入**感知损失（Perceptual Loss）**，通过预训练的VGG网络提取特征，使生成图像的特征表达在感知层面更接近真实图像。
3. **语义增强层 (Semantic Layer)**：此阶段是本项目的核心创新点。我们引入 **Transformer 模块**来建模全局信息，并结合 **CLIP 文本编码器**，实现文本驱动的语义引导。用户可以通过自然语言描述（Prompt），如“一只猫的清晰照片”，来引导模型在最终增强阶段优化图像的语义一致性和风格。此阶段的损失函数将包含 CLIP Loss，以确保图像内容与文本描述高度对齐。

该分层设计将复杂的 SR 任务解耦，使得每一层的训练目标更明确，有助于提升模型的稳定性和最终效果，并为过程可视化提供了便利。

#### **1.3 应用前景**

本项目的研究成果具备广阔的应用前景：

- **消费电子**：提升手机、相机拍摄照片的清晰度，优化数字变焦效果。
- **高清视频**：将老旧低分辨率的影视资料修复为 4K/8K 高清视频，提升观看体验。
- **智能安防**：增强监控视频中关键目标的清晰度，如人脸和车牌识别。
- **创意设计**：结合文本引导功能，可用于生成具有特定艺术风格（如油画、赛博朋克）的高清图像，服务于艺术创作和内容生成领域。

---

### **第 2 章 研究内容**

#### **2.1 研究内容概述**

本项目的主要研究内容包括：

1. **调研**：系统性地调研和梳理基于深度学习的图像超分辨率技术，特别是基于 Transformer 的主流模型（如 SwinIR, NAT）。
2. **设计**：设计一个创新的三阶段式 SR 模型。该模型以 CNN 构建轮廓和纹理层，以 Transformer 构建语义增强层，并融合 CLIP 实现文本提示（Prompt）引导功能。
3. **实现**：使用 Python 和 PyTorch 框架，完整实现上述模型的各个模块、分层损失函数以及完整的训练/推理管线。
4. **评估**：在 Set5、Set14、Urban100 等公开标准数据集上对模型进行训练和测试，使用峰值信噪比（PSNR）和结构相似性（SSIM）作为客观评价指标，并结合主观视觉效果进行综合评估。
5. **优化**：根据实验结果，对模型结构、超参数及损失函数权重进行分析与调优，并探索模型轻量化部署的可能性。

#### **2.2 研究目标**

- 成功构建一个以 Transformer 为核心、可通过文本提示进行语义引导的图像超分辨率模型。
- 在标准测试集上，使模型的 PSNR/SSIM 指标达到或接近当前先进（SOTA）方法的水平。
- 实现一个可视化的推理过程，能够动态展示图像从轮廓、纹理到语义增强的逐级清晰化动画（GIF）。
- 完成一份结构完整、内容详实的项目报告和一套可演示的代码系统。

#### **2.3 算法初步设计**

1. **整体架构**：采用分层渐进式结构 `StructuredSRModel`，依次包含 `ContourBlock`、`TextureBlock` 和 `SemanticBlock`。
2. **输入与输出**：模型输入为低分辨率图像和一段可选的文本 Prompt。输出为三个阶段的中间图像和最终的高分辨率图像。
3. **核心模块**：
    - `ContourBlock`: 采用基础的 CNN 结构。
    - `TextureBlock`: 采用更深的 CNN 结构。
    - `SemanticBlock`: 采用 Transformer 编码器，并利用**交叉注意力（Cross-Attention）机制**融合来自 CLIP 的文本嵌入向量 `prompt_embed`。
4. **损失函数**：采用多任务组合损失 `StructuredLoss`： Ltotal​=λ1​LL1​(Istep1​,IHR​)+λ2​Lperceptual​(Istep2​,IHR​)+λ3​LCLIP​(Istep3​,Pembed​) 其中，Istepi​​ 为各阶段输出图像，IHR​ 为真实高分辨率图像，Pembed​ 为文本嵌入向量，λi​ 为各损失项的权重系数。

---

### **第 3 章 研究方案**

#### **3.1 数据准备**

- **数据集**：选用业界公认的标准 SR 数据集，如 **Set5, Set14, BSD100, Urban100**。训练时，从这些数据集中提取图像块（patches）；测试时，在完整的图像上进行评估。
- **预处理**：编写 `SRDataset` 类，负责读取 HR 图像，并通过双三次插值等方式生成对应的 LR 图像。同时，为部分图像关联描述性文本 prompt。

#### **3.2 模型实现计划**

- **编码器**：实现 `clip_prompt_encoder.py`，使用 `transformers` 库加载预训练的 CLIP 模型，用于将文本 prompt 编码为语义向量。
- **核心模型**：实现 `structured_sr_model.py`，定义三个核心模块（Contour, Texture, Semantic）及完整的前向传播逻辑。
- **损失函数**：实现 `structured_loss.py`，组合 L1 损失、VGG 感知损失和 CLIP 图文对比损失。

#### **3.3 模型训练计划**

- **训练脚本**：实现 `train_structured.py` 作为主训练逻辑。
- **优化器**：选用 AdamW 优化器，它在 Transformer 模型上表现更佳。
- **训练策略**：采用端到端的方式训练整个模型，通过反向传播统一优化所有参数。定期在验证集上评估性能，并保存最优模型权重。

#### **3.4 模型评估与调优**

- **客观评估**：编写评估脚本，批量计算测试集上输出图像的 PSNR 和 SSIM 指标，并与基线模型（如 EDSR, SwinIR）进行对比。
- **主观评估**：实现 `structure_gif.py`，将模型三个阶段的输出合成为 GIF 动画，直观地展示图像逐步变清晰的过程，用于定性分析和成果展示。
- **调优**：根据评估结果，调整损失函数权重、学习率、网络深度等超参数。

#### **3.5 开发时间计划**

本项目的开发周期将严格遵循课程安排 ：

- **第15周 (6.2 - 6.8)**：完成项目选题、技术方案深度调研与设计，完成并提交本开题报告 。
    
- **第16周 (6.9 - 6.15)**：搭建 Conda 开发环境 ，完成数据准备与加载模块。初步实现 `ContourBlock` 和 `TextureBlock`，并进行单元测试。
    
- **第17周 (6.16 - 6.18)**：实现核心的 `SemanticBlock`（Transformer 模块）和完整的 `StructuredLoss`。完成训练与评估主流程代码，开始模型训练，并进行初步的调优。
- **6.18 日前**：完成所有代码的调试与整理，撰写最终的结题报告，并提交代码与报告 。
    
- **6.19 - 6.22 日**：制作答辩 PPT，准备演示 DEMO，参加最终的现场答辩评审 。
    

---

### **第 4 章 研究基础与工作条件**

#### **4.1 研究基础**

- **知识储备**：团队成员均已学习过《Python程序设计》、《深度学习》等相关课程，具备扎实的编程能力和理论基础，熟悉 PyTorch 深度学习框架。
- **技术积累**：团队已通过前期深入的学习和调研，完成了一份详细的技术实现方案（见《学习记录.md》），对项目的核心技术、代码结构和实现路径已有清晰的规划。

#### **4.2 实验条件**

- **硬件环境**：团队成员配备有搭载 NVIDIA GeForce RTX 30/40 系列 GPU 的个人电脑，可满足本项目模型的训练需求。
- **软件环境**：
    - **操作系统**：Windows / Linux
    - **开发语言**：Python 3.9+
    - **核心框架**：PyTorch, Transformers
    - **依赖库**：NumPy, Pillow, imageio, tqdm 等
    - **环境管理**：使用 Conda 创建独立的虚拟环境 `sr-transformer`，以确保依赖的隔离与一致性。