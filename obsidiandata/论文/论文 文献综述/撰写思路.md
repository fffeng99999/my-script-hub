## 论文标题：
Transformer层级解释性与训练策略优化

## 撰写思路：
围绕以下主线逻辑进行整理：
结构本体：Transformer 层是否具有可分功能？→ Rogers 2020, Voita 2019, Addition in Transformers, DecoderLens, Concept ViT
分析机制：我们如何解释不同层的行为？→ AttnLRP, DecoderLens, Hierarchical Filtering, Optimizing Attribution, LRP
解释发现：层级行为是否支持结构泛化？→ Ahuja 2025, Hierarchical Filtering, Rogers 2020
优化实践：如何基于层解释性做结构优化？→ Fan 2019, Houlsby 2019, Analyzing Layer Similarity, IA-RED²
训练启发：可解释性是否反哺训练策略？→ LR-Drop, IA-RED², Houlsby 2019, AttnLRP + structured pruning
实验方法标准化：对层行为的可控分析设计？→ Dataset Interfaces, Learning Syntax Without Planting Trees