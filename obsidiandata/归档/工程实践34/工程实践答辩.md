工程实践答辩

我来说明这个详细逻辑,先分为几个模块

1.首先我们是如何处理我们的数据集COCO的呢,在命令行中我们需要输入各种参数,其中的hr_dir代表hr图片的目录,读取完这个目录的照片后,我们先对所有图片做随机化,打乱索引,然后选取总图片数的90%作为train数据,其余的10%作为val数据(用于训练时实时测试效果),我们对于训练图片集进行处理,首先我们知道coco的高清图大多为640*480的图,我们先对这些hr图进行居中裁剪,也就是裁剪图片中心固定大小(patch_size由输入参数决定),然后对裁剪的这部分进行下采样得到lr图片,然后将这部分转化为tensor张量,最后再归一化,(在处理数据的这部分,也就是sr_dataset.py中我们还实现了可选的模拟图像退化,如添加噪声,高斯噪声的函数,可以看训练效果要求增加),这里作为第一部分

2.然后处理完数据后,形成归一化的train张量集和归一化的val张力集,我们使用Unet作为生成器,命名为G,用batchGAN作为判别器D,作用为,将lr图输入生成器G中得到一张SR图,而判别器D来判别这样sr图和原来的hr图谁是真的原图,这样的对抗设计,可以帮助生成器更加细致的恢复局部结构的质量,接着对G和D分别设置优化器(其作用是优化G和D的参数,让其更好的完成自己的工作,使用反向传播更新参数),都使用Adam算法(使用它是因为它适合就像这种项目中使用大量非线性结构的如,unet,判别器),最后在训练之前,我们设置一个自适应学习率调整的逻辑,用于在训练过程中动态调整生成器G的学习率(原理是如果5轮训练后,PSNR没有上升,那么就减半G的学习率)


3.然后进入训练逻辑,对于每一轮训练中,我们每一次选取一个图像对(lr+hr),对于每一个图像对,我们这样处理,先用给G输入lr图,生成对应的SR图,然后使用判别器D来判断原来的hr和G生成的sr图,谁是真的hr图(以patch为单位而不是整图),判断完毕后在反向传播,使用优化器更新D的参数(先训练更新D),然后再一次用G生成一张sr图,计算G的loss(包括两部分,L1loss加上TVloss),然后反向传播更新G的参数


4.最后,更新完一个图像对的G和D的参数后,进行相关的指标统计,评估计算以及TensorBoard可视化和模型保存工作,首先我们把上面用G生成的sr图和原来的hr图反归一化(反归一化后才能计算PSNR和SSIM),然后计算对应的PSNR和SSIM,然后将他们加到一个总的PSNR和SSIM上用于后续算平均,然后依据上面计算的各类loss以及PSNR和SSIM,写入tensorboard中实现可视化,然后基于这一个的结果和验证集中每一个的照片去计算对应的PSNR和SSIM看效果如何,最终的val集中的PSNR和SSIM加和后算平均得到最终的PSNR和SSIM,最后保留验证集中最高PSNR的一次作为模型参数,最后使用上面提到的学习率调节(依据PSNR指标)来调节学习率


其中第1部分指一个箭头,指向dataset.py,其中说明如何处理图片数据

其中第2部分指两个箭头,指向unet模型的逻辑,以及patchGAN的实现逻辑

最后第4部分指向metrics.py指向评价指标PSNR,SSIM如何计算









对于unet部分,用例子说明原理我们输入一张 [1, 3, 64, 64] 的低分辨率图像张量（batch=1, RGB通道=3，高宽=64×64），首先经过 DoubleConv 提取基础特征，输出尺寸仍是 [1, 64, 64, 64]。接着进入 五层下采样（Down） 模块，每一层通过 MaxPool2d(2) 将空间尺寸缩小一半，并加深通道数：从 64×64 到 32×32、16×16、8×8、4×4，最后变为 [1, 1024, 2, 2]（假设 base_channels 为 64）。然后进入 五层上采样（Up），逐层恢复空间尺寸，每一层通过 PixelShuffle 或转置卷积扩大一倍：2→4→8→16→32→64，最终恢复为 [1, 64, 64, 64]。在每一层上采样中，会拼接跳跃连接对应的下采样特征，进行融合与增强。再经过 OutConv 映射到输出通道数（RGB 的 3 通道），得到 [1, 3, 64, 64] 的初步超分结果。

随后进入 SEBlock，对这 3 个通道加权强调重要通道，保持尺寸不变。接下来通过 F.interpolate 以双线性插值将图像从 64×64 放大为最终的超分大小（例如放大 4 倍后为 256×256）。如果启用了 use_residual=True，还会将原始输入图像上采样到相同尺寸并加在输出上形成残差连接，进一步提升重建的稳定性与保真度。最后，通过 RefineBlock 去除图像中过度锐化的伪细节，输出最终的超分图像 [1, 3, 256, 256]。我们这里相比原始的UNET,层数加到了5层,还增加了SEBlock机制,用于让模型学会关注哪些通道更重要,不对所有通道同等看待,而是自适应增强有价值的通道,提高细节恢复能力,另一个新的东西就是RefineBlock,用轻量的卷积微调结果,抑制边缘锐利,还有一个可以选择的残差连接,它的作用是将原始的LR图片直接通过差值为HR的大小,学习残差结构,提高稳定性

对于判别器D的原理,也就是patchGAN的原理,我们举例说明以一张大小为 256×256 的 RGB 图像为例，PatchGAN 的判别,我们首先将这张图像输入到 PatchGAN 判别器中，该判别器通过连续的 4×4 卷积层对图像进行逐层下采样，每一层都将图像的空间尺寸缩小一半，直到最终输出一个大小为 16×16 的打分图。这个打分图的每个像素点并不对应原图中的一个像素，而是对应原图中的一个小区域（一个 patch大小）。每个位置上的值代表判别器对这个 patch 是“真实的 HR 图像”还是“生成的伪图像”的打分结果，因此整个输出可以理解为对图像各个局部区域进行真实性判断的图。在训练时，生成器的目标就是让这些局部判别分数尽可能接近于“真图”标签，从而生成不仅在整体上看起来合理，而且在局部细节上也足够真实的图像。这种方法比起只对整图判断更加细致，也能有效提升图像的纹理质量