Jiang J, Zhou J, Zhu Z. Tracing Representation Progression: Analyzing and Enhancing Layer-Wise Similarity[C]//The Thirteenth International Conference on Learning Representations. 2024.

Achtibat R, Hatefi S M V, Dreyer M, et al. Attnlrp: attention-aware layer-wise relevance propagation for transformers[J]. arXiv preprint arXiv:2402.05602, 2024.

Langedijk A, Mohebbi H, Sarti G, et al. Decoderlens: Layerwise interpretation of encoder-decoder transformers[J]. arXiv preprint arXiv:2310.03686, 2023.

Dorszewski T, Tětková L, Jenssen R, et al. From colors to classes: Emergence of concepts in vision transformers[J]. arXiv preprint arXiv:2503.24071, 2025.

Garnier-Brun J, Mézard M, Moscato E, et al. How transformers learn structured data: insights from hierarchical filtering[J]. arXiv preprint arXiv:2408.15138, 2024.

Pan B, Panda R, Feris R S, et al. Interpretability-aware redundancy reduction for vision transformers: U.S. Patent 12,154,307[P]. 2024-11-26.

Hatefi S M V, Dreyer M, Achtibat R, et al. Pruning by explaining revisited: Optimizing attribution methods to prune cnns and transformers[C]//European Conference on Computer Vision. Springer, Cham, 2025: 152-169.

Quirke P, Barez F. Understanding addition in transformers[J]. arXiv preprint arXiv:2310.13121, 2023.


Ahuja K, Balachandran V, Panwar M, et al. Learning Syntax Without Planting Trees: Understanding Hierarchical Generalization in Transformers[J]. Transactions of the Association for Computational Linguistics, 2025, 13: 121-141.

Assran M, Duval Q, Misra I, et al. Self-supervised learning from images with a joint-embedding predictive architecture[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 15619-15629.

Fan A, Grave E, Joulin A. Reducing transformer depth on demand with structured dropout[J]. arXiv preprint arXiv:1909.11556, 2019.

Houlsby N, Giurgiu A, Jastrzebski S, et al. Parameter-efficient transfer learning for NLP[C]//International conference on machine learning. PMLR, 2019: 2790-2799.

Rogers A, Kovaleva O, Rumshisky A. A primer in BERTology: What we know about how BERT works[J]. Transactions of the association for computational linguistics, 2021, 8: 842-866.

Vendrow J, Jain S, Engstrom L, et al. Dataset interfaces: Diagnosing model failures using controllable counterfactual generation[J]. arXiv preprint arXiv:2302.07865, 2023.

Voita E, Talbot D, Moiseev F, et al. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned[J]. arXiv preprint arXiv:1905.09418, 2019.

Rogers A, Kovaleva O, Rumshisky A. A primer in BERTology: What we know about how BERT works[J]. Transactions of the association for computational linguistics, 2021, 8: 842-866.

Ahuja K, Balachandran V, Panwar M, et al. Learning syntax without planting trees: Understanding when and why transformers generalize hierarchically[C]//ICML 2024 Workshop on Mechanistic Interpretability. 2024.

Stickland A C, Murray I. Bert and pals: Projected attention layers for efficient adaptation in multi-task learning[C]//International Conference on Machine Learning. PMLR, 2019: 5986-5995.

Tenney I, Das D, Pavlick E. BERT rediscovers the classical NLP pipeline[J]. arXiv preprint arXiv:1905.05950, 2019.

Batziou E, Bichler M, Fichtl M. Core-stability in assignment markets with financially constrained buyers[C]//Proceedings of the 23rd ACM Conference on Economics and Computation. 2022: 473-474.

|Ametova E, Burca G, Chilingaryan S, et al. Crystalline phase discriminating neutron tomography using advanced reconstruction methods[J]. Journal of Physics D: Applied Physics, 2021, 54(32): 325502.|
|MLA||

Sanh V, Debut L, Chaumond J, et al. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter[J]. arXiv preprint arXiv:1910.01108, 2019.

Ross A S, Hughes M C, Doshi-Velez F. Right for the right reasons: Training differentiable models by constraining their explanations[J]. arXiv preprint arXiv:1703.03717, 2017.

McCann B, Keskar N S, Xiong C, et al. The natural language decathlon: Multitask learning as question answering[J]. arXiv preprint arXiv:1806.08730, 2018.

Punia R, Schenk N, Chiarcos C, et al. Towards the first machine translation system for Sumerian transliterations[C]//Proceedings of the 28th International Conference on Computational Linguistics. 2020: 3454-3460.

Chefer H, Gur S, Wolf L. Transformer interpretability beyond attention visualization[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 782-791.

Kumar S, Carley K M. Tree lstms with convolution units to predict stance and rumor veracity in social media conversations[C]//Proceedings of the 57th annual meeting of the association for computational linguistics. 2019: 5047-5058.

Kennedy B, Jin X, Davani A M, et al. Contextualizing hate speech classifiers with post-hoc explanation[J]. arXiv preprint arXiv:2005.02439, 2020.

Ferrando J, Gállego G I, Costa-Jussà M R. Measuring the mixing of contextual information in the transformer[J]. arXiv preprint arXiv:2203.04212, 2022.

Wang W, Tu Z. Rethinking the value of transformer components[J]. arXiv preprint arXiv:2011.03803, 2020.

Noci L, Anagnostidis S, Biggio L, et al. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse[J]. Advances in Neural Information Processing Systems, 2022, 35: 27198-27211.

Goerttler T, Obermayer K. Similarity of pre-trained and fine-tuned representations[J]. arXiv preprint arXiv:2207.09225, 2022.

Yang Y, Wipf D P. Transformers from an optimization perspective[J]. Advances in Neural Information Processing Systems, 2022, 35: 36958-36971.

Ali A, Schnake T, Eberle O, et al. XAI for transformers: Better explanations through conservative propagation[C]//International conference on machine learning. PMLR, 2022: 435-451.

