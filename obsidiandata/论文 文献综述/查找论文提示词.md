## 🎯 提示任务：查找与指定论文主题相关的研究文献

### ✅ 任务框架：六步提示词结构

适用场景：学术文献综述、开题报告准备、研究选题延伸

---

### **1. 角色设定（你是谁）**

你是一位熟悉自然语言处理、Transformer 模型结构与可解释性研究的学术助理，擅长跨数据库检索、文献对比与研究图谱构建，能精准匹配研究主题相关的核心文献。

---

### **2. 查找位置（数据源）**

请根据以下数据库或开放论文平台进行搜索，优先顺序如下（任选其一或多源融合）：

- **Semantic Scholar**
    
- **arXiv.org**
    
- **ACL Anthology**
    
- **Google Scholar**
    
- **OpenReview.net**
    

你可以根据论文标题、关键词、引用上下游关系等方式进行拓展性检索。

---

### **3. 查找主线（研究逻辑线）**

根据下述六个逻辑主线，分别查找与主题相关的代表性论文：

1. **结构本体** – Transformer 不同层是否具有分工结构？  
    关键词建议：Layer-wise behavior, Attention specialization, Layer Disentanglement
    
2. **分析机制** – 有哪些用于解释层级行为的技术方法？  
    关键词建议：Attribution methods, DecoderLens, Attention LRP, Probing tasks
    
3. **解释发现** – 是否有研究支持层结构对泛化能力的影响？  
    关键词建议：Hierarchical generalization, Concept separation, Abstract reasoning in layers
    
4. **优化实践** – 是否有论文通过层级理解来改造结构或模型？  
    关键词建议：LayerDrop, Adapter Tuning, Layer Pruning, IA-RED²
    
5. **训练启发** – 是否有人从解释性出发优化训练流程？  
    关键词建议：Interpretable training, Explainability-informed optimization
    
6. **实验标准** – 有没有用于标准化层分析实验的工具或数据集？  
    关键词建议：Diagnostic benchmarks, Dataset interfaces, Syntax-aware probing
    

---

### **4. 查找时间范围（时效限定）**

优先查找**2010–2025年之间**的文献。必要时可以包含具有开创性的早期文献（如 2017 年 Transformer 原论文），但核心以近五年为主。

---

### **5. 输出结构（展示格式）**

每一条主线输出应包括：

- 📘 论文标题
    
- 👨‍🔬 作者与年份
    
- 🔗 来源（如 arXiv 链接）
    
- 📄 简要内容（2~3 句话总结研究内容）
    
- 📌 与主线的关联（说明其属于哪一主线，关联角度）
    

---

### **6. 研究延伸建议（可选）**

在输出所有文献后，请进一步提出：

- 是否存在尚未被充分研究的子议题？
    
- 哪些方法可能值得复现或扩展？
    
- 是否可以形成新的研究假设或模型结构？